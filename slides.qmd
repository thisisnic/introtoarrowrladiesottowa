---
logo: "images/logo.png"
execute:
  echo: true
format:
  revealjs: 
    theme: default
    slide-number: true
    footer: "https://github.com/thisisnic/introtoarrowworkshop"
engine: knitr
---

## Introduction to Arrow in R

![](images/logo.png){fig-align="center" width="500" height="500"}

## Intros

* Stephanie Hazlitt
* Nic Crane

Arrow contributors!

## Welcome!

Today we're going to cover:

- Working with larger-than-memory datasets with Arrow
- How to get the best performance out in your analyses   
- Where to find more information   

## Workshop format

-   Slides available at https://tinyurl.com/introtoarrow 
-   Follow-along coding
-   Gaps to ask questions

::: {.notes}
- feel free to ask questions as we go along!
:::

## Getting set up

![](images/newproj.png)

Repository URL: https://github.com/thisisnic/introtoarrowworkshop 

## Dataset to follow along with

```{r}
#| label: get-data
#| eval: false
options(timeout = 1800)
download.file(
  url = "https://r4ds.s3.us-west-2.amazonaws.com/seattle-library-checkouts.csv",
  destfile = "./data/seattle-library-checkouts.csv"
)
```

::: {.notes}
- dataset from R For Data Science
- run this from within the project you've got set up
- first line of code increase the download timeout to 30 mins - essential when downloading larger datasets
- if you don't have the data yet, download it now - next section talking about arrow
:::

## Backup option - Posit Cloud & Tiny Dataset

# Part 1 - Arrow 

::: {.notes}
- any questions before we get started?
- this section will cover a bit of background info about arrow
:::

## What is Apache Arrow?

::: columns
::: {.column width="50%"}
> A multi-language toolbox for accelerated data interchange and in-memory processing
:::

::: {.column width="50%"}
> Arrow is designed to both improve the performance of analytical algorithms and the efficiency of moving data from one system or programming language to another
:::
:::

::: {style="font-size: 70%;"}
<https://arrow.apache.org/overview/>
:::

::: {.notes}
- TODO: add main points want to hit here
:::

## Apache Arrow Specification

In-memory columnar format: a standardized, language-agnostic specification for representing structured, table-like data sets in-memory.

<br>

![](images/arrow-rectangle.png){.absolute left="200"}

::: {.notes}
- TODO: can we highlight "columnar", "standardized", "language-agnostic", and "table-like datasets" in different colours?
- brief overview of these points but coming back to later
:::

## A Multi-Language Toolbox

![](images/arrow-libraries-structure.png)

## Accelerated Data Interchange

![](images/data-interchange-with-arrow.png)

::: {.notes}
- standardisation means multiple things speaking arrow prevents copying back and forth
:::

## Accelerated In-Memory Processing

Arrow's Columnar Format is Fast

![](images/columnar-fast.png){.absolute top="120" left="200" height="600"}

::: {.notes}
- intro the toy dataset
- memory buffers are a 1-dimension structure, not like a 2D table/data.frame
- walk through this in the 2 diagrams
- analytics workflows typically have filtering, grouping by columns etc; give examples
- faster to scan adjacent areas than picking through it all + taking advantage of vectorization available on modern processes speeds up faster
:::


## arrow üì¶

<br>

![](images/arrow-r-pkg.png){.absolute top="0" left="300" width="700" height="900"}

::: {.notes}
- TODO: highlight sections with overlay colour (diff colours)
:::

## arrow üì¶

![](images/arrow-read-write-updated.png)

::: {.notes}
- different types of objects
- different file formats
- different storage locations
:::

# Part 2 - Working with Arrow Datasets

::: {.notes}
- any questions so far?
- maybe a poll - who's worked with dplyr/arrow/parquet before?
:::

## Seattle<br>Checkouts<br>Big CSV

![](images/seattle-checkouts.png){.absolute top="0" left="300"}

::: {.notes}
- TODO: add link to website
- TODO: find out for below how much of the file Arrow actually scans
- TODO: add an nrow to this section
- LIVE CODING
  - path to the data depending on where you've downloaded it
  - what is fs?
  - impact of data size on performance
  - walk through subcomponents of the output when print the `seattle_csv` object
    - explicitly mention the term "schema"
    - string and character are direct equivalents
    - why we have e.g. 64-bit integers
    - arrow automatically handles the conversion between R and Arrow data types
  - these types have been guessed from first 1MB of rows
  - can anyone spot something odd here?
  - what is an ISBN; what is the null type?
  - pulling out the schema
  - updating the schema
  - using the e.g. `string()` functions to create different data types and where in the docs?
  - show *both* schema updating and `col_types`
  - checking out the new schema
  - how often is this necessary? Often not. Good practice. Only important with CSVs not Parquet.
  - run `glimpse()` but remember it'll take a moment
    - TODO: find something to talk about here
    - 42 millions rows
    - UsageClass column data
    - ISBN column data
    - PublicationYear + why it's a string
- Recap this section!
- Questions?

:::

## How big is the dataset?

```{r}
#| label: setup
#| echo: false
#| output: false
library(arrow)
library(dplyr)
```

```{r}
#| label: check-file-size
fs::file_size("./data/seattle-library-checkouts.csv")
```

## Opening in Arrow

```{r}
#| label: open-dataset
seattle_csv <- open_dataset(
  sources = "./data/seattle-library-checkouts.csv", 
  format = "csv"
)
```

## Extract schema

```{r}
#| label: extract-schema
schema(seattle_csv)
```

## Arrow Data Types

Arrow has a rich data type system, including direct analogs of many R data types

-   `<dbl>` == `<double>`
-   `<chr>` == `<string>` or `<utf8>`
-   `<int>` == `<int32>`

<br>

<https://arrow.apache.org/docs/r/articles/data_types.html>

## Parsing the Metadata

<br>

Arrow scans üëÄ a few thousand rows of the file(s) to impute or "guess" the data types

::: {style="font-size: 80%; margin-top: 200px;"}
üìö arrow vs readr blog post: <https://thisisnic.github.io/2022/11/21/type-inference-in-readr-and-arrow/>
:::

## Parsers Are Not Always Right

```{r}
#| label: seattle-schema-again
schema(seattle_csv)
```

![](images/data-dict.png){.absolute top="300" left="330" width="700"}

::: notes
International Standard Book Number (ISBN) is a 13-digit number that uniquely identifies books and book-like products published internationally.

Data Dictionaries, metadata in data catalogues should provide this info.
:::

## Let's Control the Schema

<br>

Creating a schema manually:

```{r}
#| label: seattle-schema-write
#| eval: false
schema(
  UsageClass = utf8(),
  CheckoutType = utf8(),
  MaterialType = utf8(),
  ...
)
```

<br>

This will take a lot of typing with 12 columns üò¢

## Let's Control the Schema

```{r}
#| label: open-dataset-schema
seattle_csv <- open_dataset(
  sources = "./data/seattle-library-checkouts.csv", 
  col_types = schema(ISBN = string()),
  format = "csv"
)
```

## Previewing the data

```{r}
#| label: glimpse
seattle_csv |> glimpse()
```

# Part 3 - Data Manipulation with Arrow

::: {.notes}
- <Up to here was about 28 mins in first practice run (no time for questions)>
- Question - how many people here have used dbplyr to connect to a database in R?
:::

## Arrow dplyr backend

![](images/dplyr-backend.png)

::: {.notes}
LIVE CODING
  - Data contains book, ebooks, things which aren't book
  - Do *not* call collect() after first query - talk about lazy eval and show the query
    - endsWith to ends_with; this is actually an arrow C++ lib func
  - Don't want to pull all into memory as it's a lot of data; preview using head
  - Now look at the outputs of that
  - Next: How many books and ebooks were checked out each year?
  - Don't need to call head() to preview as the data returned is just a row for each year 
  - Run again with a timer set, then walk through results
  - Walk through the data, drop in 2020 - pandemic?
  - Not bad, it can be faster and this is what we'll talk about in part 3
RECAP
  - Any questions?
:::

## Querying the data - new column: is this a book?

```{r}
#| label: query
seattle_csv |>
  mutate(IsBook = endsWith(MaterialType, "BOOK")) |>
  select(MaterialType, IsBook)
```



## Preview the query

```{r}
#| label: preview-query
#| code-line-numbers: "|2,5"
seattle_csv |>
  head(20) |>
  mutate(IsBook = endsWith(MaterialType, "BOOK")) |>
  select(MaterialType, IsBook) |>
  collect()
```

## How many books were checked out each year?

```{r}
#| label: seattle-data-manip
seattle_csv |>
  filter(endsWith(MaterialType, "BOOK")) |>
  group_by(CheckoutYear) |>
  summarise(Checkouts = sum(Checkouts)) |>
  arrange(CheckoutYear) |> 
  collect()
```

## 9GB CSV file + arrow + dplyr

```{r}
#| label: seattle-dplyr
seattle_csv |>
  filter(endsWith(MaterialType, "BOOK")) |>
  group_by(CheckoutYear) |>
  summarise(Checkouts = sum(Checkouts)) |>
  arrange(CheckoutYear) |> 
  collect()
```

## 9GB CSV file + arrow + dplyr

```{r}
#| label: seattle-dplyr-timed
#| code-line-numbers: "6"
seattle_csv |>
  filter(endsWith(MaterialType, "BOOK")) |>
  group_by(CheckoutYear) |>
  summarise(Checkouts = sum(Checkouts)) |>
  arrange(CheckoutYear) |> 
  collect() |>
  system.time()
```

42 million rows -- not bad, but could be faster....

# Part 4 - Engineering the Data

## File Format: Apache Parquet

![](images/apache-parquet.png){.absolute top="100" left="200" width="700"}

::: {style="font-size: 60%; margin-top: 450px;"}
<https://parquet.apache.org/>
:::

## Parquet

-   usually smaller than equivalent CSV file
-   rich type system & stores the data type along with the data
-   "column-oriented" == better performance over CSV's row-by-row
-   "row-chunked" == work on different parts of the file at the same time or skip some chunks all together

::: notes
-   efficient encodings to keep file size down, and supports file compression, less data to move from disk to memory
-   CSV has no info about data types, inferred by each parser
:::

## Parquet Files: "row-chunked"

![](images/parquet-chunking.png)

## Parquet Files: "row-chunked & column-oriented"

![](images/parquet-columnar.png)

## Writing to Parquet

```{r}
#| label: seattle-write-parquet-setup
#| output: false
#| echo: false
seattle_parquet <- "./data/seattle-library-checkouts-parquet"
```

```{r}
#| label: seattle-write-parquet-single
#| eval: false
seattle_parquet <- "./data/seattle-library-checkouts-parquet"

seattle_csv |>
  write_dataset(path = seattle_parquet,
                format = "parquet")
```

## Storage: Parquet vs CSV

```{r}
#| label: seattle-single-parquet-size
file <- list.files(seattle_parquet)
file.size(file.path(seattle_parquet, file)) / 10**9
```

<br>

Parquet about half the size of the CSV file on-disk üíæ

## File Storage:<br>Partitioning

<br>

::: columns
::: {.column width="50%"}
Dividing data into smaller pieces, making it more easily accessible and manageable
:::

::: {.column width="50%"}

```{r}
#| label: show_dir_tree_1
fs::dir_tree("data/seattle-library-checkouts/")
```

:::
:::

::: notes
also called multi-files or sometimes shards
:::

## Poll: Partitioning?

Have you partitioned your data or used partitioned data before today?

-   1Ô∏è‚É£ Yes
-   2Ô∏è‚É£ No
-   3Ô∏è‚É£ Not sure, the data engineers sort that out!

## Art & Science of Partitioning

<br>

-   avoid files \< 20MB and \> 2GB
-   avoid \> 10,000 files (ü§Ø)
-   partition on variables used in `filter()`

::: notes
-   guidelines not rules, results vary
-   experiment
-   arrow suggests avoid files smaller than 20MB and larger than 2GB
-   avoid partitions that produce more than 10,000 files
-   partition by variables that you filter by, allows arrow to only read relevant files
:::

## Rewriting the Data Again

```{r}
#| label: seattle-write-partitioned
#| eval: false
seattle_parquet_part <- "./data/seattle-library-checkouts"

seattle_csv |>
  group_by(CheckoutYear) |>
  write_dataset(path = seattle_parquet_part,
                format = "parquet")
```

## What Did We "Engineer"?

```{r}
#| label: seattle-partitioned-sizes
seattle_parquet_part <- "./data/seattle-library-checkouts"

sizes <- tibble(
  files = list.files(seattle_parquet_part, recursive = TRUE),
  size_GB = file.size(file.path(seattle_parquet_part, files)) / 10**9
)

sizes
```

## 4.5GB partitioned Parquet files + arrow + dplyr

```{r}
#| label: seattle-partitioned-dplyr-timed
seattle_parquet_part <- "./data/seattle-library-checkouts"

open_dataset(seattle_parquet_part,
             format = "parquet") |>
  filter(endsWith(MaterialType, "BOOK")) |>
  group_by(CheckoutYear) |>
  summarise(Checkouts = sum(Checkouts)) |>
  arrange(CheckoutYear) |> 
  collect() |>
  system.time()
```

<br>

42 million rows -- not too shabby!

## Partition Design

::: columns
::: {.column width="50%"}
-   Partitioning on variables commonly used in `filter()` often faster
-   Number of partitions also important (Arrow reads the metadata of each file)
:::

::: {.column width="50%"}
```{r}
#| label: show_dir_tree
fs::dir_tree("data/seattle-library-checkouts/")
```
:::
:::

## Performance Review: Single CSV

How long does it take to calculate the number of books checked out in each month of 2021?

<br>

```{r}
#| label: seattle-single-csv-dplyr-timed
open_dataset(
  sources = "./data/seattle-library-checkouts.csv", 
  format = "csv"
) |> 
  filter(CheckoutYear == 2021, endsWith(MaterialType, "BOOK")) |>
  group_by(CheckoutMonth) |>
  summarize(TotalCheckouts = sum(Checkouts)) |>
  arrange(desc(CheckoutMonth)) |>
  collect() |>
  system.time()
```

## Performance Review: Partitioned Parquet

How long does it take to calculate the number of books checked out in each month of 2021?

<br>

```{r}
#| label: seattle-parquet-partitioned-dplyr-timed
open_dataset("./data/seattle-library-checkouts",
             format = "parquet") |> 
  filter(CheckoutYear == 2021, endsWith(MaterialType, "BOOK")) |>
  group_by(CheckoutMonth) |>
  summarize(TotalCheckouts = sum(Checkouts)) |>
  arrange(desc(CheckoutMonth)) |>
  collect() |> 
  system.time()
```

## Engineering Data Tips for Improved Storage & Performance

<br>

-   consider "column-oriented" file formats like Parquet
-   consider partitioning, experiment to get an appropriate partition design üóÇÔ∏è
-   watch your schemas üëÄ

# Part 5 - More Resources

## Getting help and more resources

<!-- Add in some links here -->

## R for Data Science (2e)

::: columns
::: {.column width="50%"}
![](images/r4ds-cover.jpg){.absolute top="100" width="400"}
:::

::: {.column width="50%"}
<br>

[Chapter 23: Arrow](https://r4ds.hadley.nz/arrow.html)

<br>

<https://r4ds.hadley.nz/>
:::
:::

## Scaling Up with R and Arrow

::: columns
::: {.column width="50%"}
![](images/dummybookcover.png)
:::

::: {.column width="50%"}
Currently being written - preview available online soon!

<br>

<https://arrowrbook.com>
:::
:::

